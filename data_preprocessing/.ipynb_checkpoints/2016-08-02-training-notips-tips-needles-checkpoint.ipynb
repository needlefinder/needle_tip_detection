{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu\n",
      "/home/ubuntu/preprocessed_data/needles_10-10-10_1.00-1.00-1.00/\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "from __future__ import division\n",
    "import joblib\n",
    "import glob\n",
    "import os, re\n",
    "import numpy as np\n",
    "import nrrd\n",
    "import numpy as np\n",
    "from sklearn import datasets, svm, metrics, decomposition\n",
    "from sklearn.externals import joblib\n",
    "import time\n",
    "from joblib import Parallel, delayed  \n",
    "import multiprocessing\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "USERPATH = os.path.expanduser(\"~\")\n",
    "print(USERPATH)\n",
    "import six.moves.cPickle as pickle\n",
    "# import tensorflow as tf\n",
    "\n",
    "# import theano\n",
    "# theano.config.device = 'gpu'\n",
    "# theano.config.floatX = 'float32'\n",
    "\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D, ZeroPadding1D, Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers.convolutional import Convolution3D, MaxPooling3D, ZeroPadding3D\n",
    "from keras.optimizers import SGD, Adadelta, Adagrad\n",
    "from keras.utils import np_utils, generic_utils\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# server = tf.train.Server.create_local_server()\n",
    "# sess = tf.Session(server.target)\n",
    "\n",
    "# from keras import backend as K\n",
    "# K.set_session(sess)\n",
    "\n",
    "# tb = TensorBoard(log_dir='/tmp/tensorboard', histogram_freq=1, write_graph=True)\n",
    "\n",
    "\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "# checkpointer = ModelCheckpoint(filepath=\"weights2d.hdf5\", verbose=1, save_best_only=True)\n",
    "patchsize = [10,10,10]\n",
    "data_spacing = [1,1,1]\n",
    "notipsPath = USERPATH + \"/preprocessed_data/notips_%d-%d-%d_%.2f-%.2f-%.2f/\" %(tuple(patchsize)+tuple(data_spacing))\n",
    "tipsPath = USERPATH + \"/preprocessed_data/needles_%d-%d-%d_%.2f-%.2f-%.2f/\" %(tuple(patchsize)+tuple(data_spacing))\n",
    "\n",
    "casesToExclude = [64,77]\n",
    "\n",
    "\n",
    "def getTrainingPaths(tipsPath, cases=[64,77]):\n",
    "    strL = \"\"\n",
    "    for c in cases:\n",
    "        strL+=\"%03d|\"%c\n",
    "    fnames=glob.glob(tipsPath + \"/*/*.nrrd\")\n",
    "    regex=re.compile(\"^((?!%s).)*$\"%strL[:-1])\n",
    "    paths = [m.group(0) for l in fnames for m in [regex.search(l)] if m]\n",
    "    return paths\n",
    "\n",
    "def loadAllDataFromPath(path, casesToExclude):\n",
    "    # path in directorty\n",
    "    \n",
    "#     cubeTipsPath = glob.glob(path + \"/*/*.nrrd\")\n",
    "    cubeTipsPath = getTrainingPaths(path, casesToExclude)\n",
    "    # number of samples\n",
    "    N = len(cubeTipsPath)\n",
    "    \n",
    "    cubeTips = []\n",
    "    data = []\n",
    "    for path_i in cubeTipsPath:\n",
    "        cubeTips.append(nrrd.read(path_i))\n",
    "    for i in range(N):\n",
    "        # c = np.array(cubeTips[i][0])  # for patches of size 20,20,20\n",
    "        c = np.array(cubeTips[i][0][:,:,:]) # for patches of size 10,10,10\n",
    "        if c.shape==tuple(patchsize):\n",
    "            data.append(np.array(c))\n",
    "    output = np.array(data, dtype='float32')\n",
    "    print('number of sample %d' %len(output))\n",
    "    return output\n",
    "\n",
    "\n",
    "print(tipsPath)\n",
    "# tips = loadAllDataFromPath(tipsPath, casesToExclude)\n",
    "# notips = loadAllDataFromPath(notipsPath, casesToExclude)[:3*len(tips)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sample 55574\n",
      "number of sample 93906\n",
      "55574 93906\n",
      "target shape: (149480,)\n",
      "data shape: (149480, 10, 10, 10)\n"
     ]
    }
   ],
   "source": [
    "tips = loadAllDataFromPath(tipsPath, casesToExclude)\n",
    "notips = loadAllDataFromPath(notipsPath, casesToExclude)[:5*len(tips)]\n",
    "\n",
    "print(len(tips), len(notips))\n",
    "\n",
    "target_0 = [0 for i in range(len(notips))]\n",
    "target_1 = [1 for i in range(len(tips))]\n",
    "y_train = np.array(target_0 + target_1)\n",
    "print('target shape:', y_train.shape)\n",
    "X_train = np.array(list(notips)+list(tips))\n",
    "\n",
    "print('data shape:', X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape and label shape\n",
      "(149480, 10, 10, 10) (149480,)\n"
     ]
    }
   ],
   "source": [
    "o = 10\n",
    "f_Xtrain = open('X_data_n%d.save'%o, 'wb')\n",
    "f_ytrain = open('y_data_n%d.save'%o, 'wb')\n",
    "\n",
    "pickle.dump(X_train, f_Xtrain, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(y_train, f_ytrain, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "f_Xtrain.close()\n",
    "f_ytrain.close()\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "# Load the dataset\n",
    "f_Xdata = open('X_data_n%d.save'%o, 'rb')\n",
    "f_ydata = open('y_data_n%d.save'%o, 'rb')\n",
    "\n",
    "X_data_ = pickle.load(f_Xdata)\n",
    "X_data_ = X_data_.astype('float32')\n",
    "\n",
    "# normalize the raw data\n",
    "X_data_ -= np.mean(X_data_)\n",
    "X_data_ /= np.std(X_data_)\n",
    "\n",
    "## second method for normalization\n",
    "# X_data /= 255\n",
    "\n",
    "y_data= pickle.load(f_ydata)\n",
    "y_data_binary = to_categorical(y_data)\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_data)\n",
    "y_data = encoder.transform(y_data)\n",
    "\n",
    "print(\"Data shape and label shape\")\n",
    "print(X_data_.shape, y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[7]:\n",
    "\n",
    "def shuffle_in_unison_inplace(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "\n",
    "# init the global var\n",
    "model = 0\n",
    "m = 13\n",
    "conv3d = False\n",
    "conv1d = False\n",
    "dimOrdering = 'tf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(149480, 10, 10, 10)\n",
      "Epoch 1/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.4091 - acc: 0.8146    \n",
      "Epoch 2/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.2084 - acc: 0.9290    \n",
      "Epoch 3/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1799 - acc: 0.9398    \n",
      "Epoch 4/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1666 - acc: 0.9452    \n",
      "Epoch 5/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1549 - acc: 0.9510    \n",
      "Epoch 6/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1514 - acc: 0.9517    \n",
      "Epoch 7/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1435 - acc: 0.9546    \n",
      "Epoch 8/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1392 - acc: 0.9567    \n",
      "Epoch 9/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1361 - acc: 0.9577    \n",
      "Epoch 10/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1348 - acc: 0.9575    \n",
      "Epoch 11/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1330 - acc: 0.9582    \n",
      "Epoch 12/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1258 - acc: 0.9620    \n",
      "Epoch 13/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1260 - acc: 0.9621    \n",
      "Epoch 14/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1247 - acc: 0.9619    \n",
      "Epoch 15/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1215 - acc: 0.9634    \n",
      "Epoch 16/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1206 - acc: 0.9632    \n",
      "Epoch 17/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1179 - acc: 0.9643    \n",
      "Epoch 18/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1141 - acc: 0.9654    \n",
      "Epoch 19/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1136 - acc: 0.9658    \n",
      "Epoch 20/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1126 - acc: 0.9665    \n",
      "Epoch 21/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1118 - acc: 0.9665    \n",
      "Epoch 22/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1086 - acc: 0.9682    \n",
      "Epoch 23/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1117 - acc: 0.9663    \n",
      "Epoch 24/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1063 - acc: 0.9682    \n",
      "Epoch 25/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1083 - acc: 0.9677    \n",
      "Epoch 26/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1053 - acc: 0.9691    \n",
      "Epoch 27/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1027 - acc: 0.9701    \n",
      "Epoch 28/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1024 - acc: 0.9695    \n",
      "Epoch 29/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1004 - acc: 0.9709    \n",
      "Epoch 30/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1015 - acc: 0.9699    \n",
      "Epoch 31/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1013 - acc: 0.9700    \n",
      "Epoch 32/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1025 - acc: 0.9693    \n",
      "Epoch 33/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0975 - acc: 0.9713    \n",
      "Epoch 34/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0973 - acc: 0.9712    \n",
      "Epoch 35/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0959 - acc: 0.9719    \n",
      "Epoch 36/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0983 - acc: 0.9717    \n",
      "Epoch 37/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0960 - acc: 0.9718    \n",
      "Epoch 38/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0942 - acc: 0.9720    \n",
      "Epoch 39/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0923 - acc: 0.9726    \n",
      "Epoch 40/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0928 - acc: 0.9730    \n",
      "Epoch 41/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0914 - acc: 0.9726    \n",
      "Epoch 42/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0883 - acc: 0.9741    \n",
      "Epoch 43/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0883 - acc: 0.9740    \n",
      "Epoch 44/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0875 - acc: 0.9749    \n",
      "Epoch 45/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0902 - acc: 0.9735    \n",
      "Epoch 46/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0903 - acc: 0.9734    \n",
      "Epoch 47/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0854 - acc: 0.9751    \n",
      "Epoch 48/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0869 - acc: 0.9745    \n",
      "Epoch 49/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0845 - acc: 0.9750    \n",
      "Epoch 50/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0863 - acc: 0.9747    \n",
      "Epoch 51/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0852 - acc: 0.9746    \n",
      "Epoch 52/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0878 - acc: 0.9741    \n",
      "Epoch 53/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0835 - acc: 0.9751    \n",
      "Epoch 54/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0810 - acc: 0.9757    \n",
      "Epoch 55/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0825 - acc: 0.9757    \n",
      "Epoch 56/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0811 - acc: 0.9756    \n",
      "Epoch 57/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0804 - acc: 0.9762    \n",
      "Epoch 58/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0801 - acc: 0.9761    \n",
      "Epoch 59/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0799 - acc: 0.9767    \n",
      "Epoch 60/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0774 - acc: 0.9769    \n",
      "Epoch 61/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0779 - acc: 0.9771    \n",
      "Epoch 62/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0784 - acc: 0.9769    \n",
      "Epoch 63/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0788 - acc: 0.9769    \n",
      "Epoch 64/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0791 - acc: 0.9765    \n",
      "Epoch 65/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0783 - acc: 0.9765    \n",
      "Epoch 66/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0781 - acc: 0.9765    \n",
      "Epoch 67/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0734 - acc: 0.9783    \n",
      "Epoch 68/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0750 - acc: 0.9782    \n",
      "Epoch 69/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0758 - acc: 0.9779    \n",
      "Epoch 70/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0727 - acc: 0.9784    \n",
      "Epoch 71/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0736 - acc: 0.9791    \n",
      "Epoch 72/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0751 - acc: 0.9776    \n",
      "Epoch 73/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0721 - acc: 0.9791    \n",
      "Epoch 74/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0730 - acc: 0.9786    \n",
      "Epoch 75/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0732 - acc: 0.9780    \n",
      "Epoch 76/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0691 - acc: 0.9795    \n",
      "Epoch 77/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0722 - acc: 0.9790    \n",
      "Epoch 78/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0697 - acc: 0.9799    \n",
      "Epoch 79/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0720 - acc: 0.9789    \n",
      "Epoch 80/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0727 - acc: 0.9783    \n",
      "Epoch 81/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0690 - acc: 0.9792    \n",
      "Epoch 82/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0753 - acc: 0.9778    \n",
      "Epoch 83/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0673 - acc: 0.9800    \n",
      "Epoch 84/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0659 - acc: 0.9805    \n",
      "Epoch 85/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0688 - acc: 0.9797    \n",
      "Epoch 86/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0707 - acc: 0.9791    \n",
      "Epoch 87/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0690 - acc: 0.9799    \n",
      "Epoch 88/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0693 - acc: 0.9795    \n",
      "Epoch 89/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0661 - acc: 0.9807    \n",
      "Epoch 90/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0683 - acc: 0.9796    \n",
      "Epoch 91/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0628 - acc: 0.9818    \n",
      "Epoch 92/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0645 - acc: 0.9811    \n",
      "Epoch 93/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0640 - acc: 0.9809    \n",
      "Epoch 94/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0671 - acc: 0.9802    \n",
      "Epoch 95/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0633 - acc: 0.9808    \n",
      "Epoch 96/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0672 - acc: 0.9801    \n",
      "Epoch 97/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0647 - acc: 0.9805    \n",
      "Epoch 98/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0617 - acc: 0.9820    \n",
      "Epoch 99/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0614 - acc: 0.9815    \n",
      "Epoch 100/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0641 - acc: 0.9810    \n",
      "74740/74740 [==============================] - 2s     \n",
      "Epoch 1/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.4062 - acc: 0.8097    \n",
      "Epoch 2/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.2079 - acc: 0.9287    \n",
      "Epoch 3/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1826 - acc: 0.9385    \n",
      "Epoch 4/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1695 - acc: 0.9451    \n",
      "Epoch 5/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1653 - acc: 0.9461    \n",
      "Epoch 6/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1557 - acc: 0.9505    \n",
      "Epoch 7/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1491 - acc: 0.9521    \n",
      "Epoch 8/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1426 - acc: 0.9552    \n",
      "Epoch 9/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1370 - acc: 0.9577    \n",
      "Epoch 10/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1338 - acc: 0.9582    \n",
      "Epoch 11/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1345 - acc: 0.9587    \n",
      "Epoch 12/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1312 - acc: 0.9597    \n",
      "Epoch 13/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1256 - acc: 0.9619    \n",
      "Epoch 14/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1234 - acc: 0.9628    \n",
      "Epoch 15/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1223 - acc: 0.9633    \n",
      "Epoch 16/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1225 - acc: 0.9628    \n",
      "Epoch 17/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1185 - acc: 0.9645    \n",
      "Epoch 18/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1172 - acc: 0.9647    \n",
      "Epoch 19/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1153 - acc: 0.9656    \n",
      "Epoch 20/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1154 - acc: 0.9655    \n",
      "Epoch 21/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1115 - acc: 0.9664    \n",
      "Epoch 22/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1103 - acc: 0.9671    \n",
      "Epoch 23/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1107 - acc: 0.9666    \n",
      "Epoch 24/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1119 - acc: 0.9666    \n",
      "Epoch 25/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1105 - acc: 0.9671    \n",
      "Epoch 26/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1071 - acc: 0.9687    \n",
      "Epoch 27/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1054 - acc: 0.9688    \n",
      "Epoch 28/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1051 - acc: 0.9688    \n",
      "Epoch 29/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1039 - acc: 0.9686    \n",
      "Epoch 30/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1082 - acc: 0.9677    \n",
      "Epoch 31/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1026 - acc: 0.9701    \n",
      "Epoch 32/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1010 - acc: 0.9705    \n",
      "Epoch 33/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1043 - acc: 0.9692    \n",
      "Epoch 34/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.1003 - acc: 0.9702    \n",
      "Epoch 35/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0991 - acc: 0.9708    \n",
      "Epoch 36/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0980 - acc: 0.9710    \n",
      "Epoch 37/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0954 - acc: 0.9720    \n",
      "Epoch 38/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0943 - acc: 0.9729    \n",
      "Epoch 39/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0970 - acc: 0.9713    \n",
      "Epoch 40/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0942 - acc: 0.9727    \n",
      "Epoch 41/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0928 - acc: 0.9726    \n",
      "Epoch 42/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0922 - acc: 0.9729    \n",
      "Epoch 43/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0923 - acc: 0.9727    \n",
      "Epoch 44/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0913 - acc: 0.9731    \n",
      "Epoch 45/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0900 - acc: 0.9740    \n",
      "Epoch 46/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0895 - acc: 0.9738    \n",
      "Epoch 47/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0925 - acc: 0.9728    \n",
      "Epoch 48/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0863 - acc: 0.9752    \n",
      "Epoch 49/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0895 - acc: 0.9740    \n",
      "Epoch 50/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0871 - acc: 0.9748    \n",
      "Epoch 51/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0856 - acc: 0.9750    \n",
      "Epoch 52/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0880 - acc: 0.9746    \n",
      "Epoch 53/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0861 - acc: 0.9750    \n",
      "Epoch 54/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0855 - acc: 0.9754    \n",
      "Epoch 55/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0866 - acc: 0.9747    \n",
      "Epoch 56/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0870 - acc: 0.9747    \n",
      "Epoch 57/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0827 - acc: 0.9759    \n",
      "Epoch 58/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0835 - acc: 0.9756    \n",
      "Epoch 59/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0837 - acc: 0.9756    \n",
      "Epoch 60/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0824 - acc: 0.9759    \n",
      "Epoch 61/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0814 - acc: 0.9764    \n",
      "Epoch 62/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0823 - acc: 0.9761    \n",
      "Epoch 63/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0793 - acc: 0.9770    \n",
      "Epoch 64/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0804 - acc: 0.9769    \n",
      "Epoch 65/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0792 - acc: 0.9776    \n",
      "Epoch 66/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0786 - acc: 0.9774    \n",
      "Epoch 67/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0786 - acc: 0.9775    \n",
      "Epoch 68/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0796 - acc: 0.9767    \n",
      "Epoch 69/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0782 - acc: 0.9774    \n",
      "Epoch 70/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0828 - acc: 0.9760    \n",
      "Epoch 71/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0759 - acc: 0.9779    \n",
      "Epoch 72/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0798 - acc: 0.9769    \n",
      "Epoch 73/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0781 - acc: 0.9772    \n",
      "Epoch 74/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0772 - acc: 0.9775    \n",
      "Epoch 75/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0759 - acc: 0.9778    \n",
      "Epoch 76/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0771 - acc: 0.9777    \n",
      "Epoch 77/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0767 - acc: 0.9778    \n",
      "Epoch 78/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0735 - acc: 0.9790    \n",
      "Epoch 79/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0706 - acc: 0.9794    \n",
      "Epoch 80/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0730 - acc: 0.9790    \n",
      "Epoch 81/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0719 - acc: 0.9790    \n",
      "Epoch 82/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0734 - acc: 0.9788    \n",
      "Epoch 83/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0727 - acc: 0.9795    \n",
      "Epoch 84/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0723 - acc: 0.9792    \n",
      "Epoch 85/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0718 - acc: 0.9792    \n",
      "Epoch 86/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0719 - acc: 0.9787    \n",
      "Epoch 87/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0719 - acc: 0.9796    \n",
      "Epoch 88/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0715 - acc: 0.9790    \n",
      "Epoch 89/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0714 - acc: 0.9790    \n",
      "Epoch 90/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0716 - acc: 0.9789    \n",
      "Epoch 91/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0728 - acc: 0.9784    \n",
      "Epoch 92/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0735 - acc: 0.9782    \n",
      "Epoch 93/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0729 - acc: 0.9795    \n",
      "Epoch 94/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0688 - acc: 0.9799    \n",
      "Epoch 95/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0674 - acc: 0.9803    \n",
      "Epoch 96/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0656 - acc: 0.9815    \n",
      "Epoch 97/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0677 - acc: 0.9804    \n",
      "Epoch 98/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0644 - acc: 0.9813    \n",
      "Epoch 99/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0674 - acc: 0.9802    \n",
      "Epoch 100/100\n",
      "74740/74740 [==============================] - 11s - loss: 0.0685 - acc: 0.9796    \n",
      "74740/74740 [==============================] - 2s     \n",
      "Standardized: 95.88% (0.11%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2863"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_baseline():\n",
    "\n",
    "    nb_classes = 1\n",
    "\n",
    "    # create model\n",
    "    global model\n",
    "    if m ==7:\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Convolution2D(10, 10, 2, border_mode='same',\n",
    "                                input_shape=(10,10,10)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Convolution2D(10, 3, 3))\n",
    "        model.add(Activation('relu'))\n",
    "        # model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.1))\n",
    "\n",
    "        model.add(Convolution2D(40, 5, 3, border_mode='same' ))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Convolution2D(40, 5, 3, border_mode='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Convolution2D(40, 5, 3))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.1))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(480))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(480))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(nb_classes))\n",
    "        model.add(Activation('sigmoid'))\n",
    "\n",
    "    if m == 11:\n",
    "        model = Sequential()\n",
    "        model.add(Convolution2D(10, 10, 10, border_mode='same',\n",
    "                                batch_input_shape=(10, 10, 10)))\n",
    "        # model.add(ZeroPadding2D((1, 1), batch_input_shape=(1, 3, 10, 10)))\n",
    "        model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_1'))\n",
    "        model.add(ZeroPadding2D((1, 1)))\n",
    "        model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_2'))\n",
    "        model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "        model.add(ZeroPadding2D((1, 1)))\n",
    "        model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_1'))\n",
    "        model.add(ZeroPadding2D((1, 1)))\n",
    "        model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_2'))\n",
    "        model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "        model.add(ZeroPadding2D((1, 1)))\n",
    "        model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_1'))\n",
    "        model.add(ZeroPadding2D((1, 1)))\n",
    "        model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_2'))\n",
    "        model.add(ZeroPadding2D((1, 1)))\n",
    "        model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_3'))\n",
    "        model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "        model.add(ZeroPadding2D((1, 1)))\n",
    "        model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_1'))\n",
    "        model.add(ZeroPadding2D((1, 1)))\n",
    "        model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_2'))\n",
    "        model.add(ZeroPadding2D((1, 1)))\n",
    "        model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_3'))\n",
    "        model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "        model.add(ZeroPadding2D((1, 1)))\n",
    "        model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_1'))\n",
    "        model.add(ZeroPadding2D((1, 1)))\n",
    "        model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_2'))\n",
    "        model.add(ZeroPadding2D((1, 1)))\n",
    "        model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_3'))\n",
    "        model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "        model.add(Flatten(input_shape=(512,3,3)))\n",
    "        model.add(Dense(480))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(480))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(nb_classes))\n",
    "        model.add(Activation('sigmoid'))\n",
    "\n",
    "    if m ==13:\n",
    "        model = Sequential()\n",
    "        model.add(Convolution2D(10, 10, 10, border_mode='same', input_shape=(10,10,10), activation='relu', name='conv1_0'))\n",
    "#         model.add(ZeroPadding2D((2, 2)))\n",
    "        model.add(Convolution2D(10, 5, 5, border_mode='same', activation='relu', name='conv1_1'))\n",
    "        model.add(Convolution2D(10, 10, 10, border_mode='same', activation='relu', name='conv1_2'))\n",
    "        model.add(MaxPooling2D((4,4), strides=(1,1)))\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(200, activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(100, activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(nb_classes, activation='sigmoid'))\n",
    "        \n",
    "    if m ==14:\n",
    "        model = Sequential()\n",
    "        model.add(Convolution2D(20, 10, 10, border_mode='same', input_shape=(10,10,20), activation='relu', name='conv1_0'))\n",
    "        model.add(Convolution2D(20, 5, 5, border_mode='same', activation='relu', name='conv1_1'))\n",
    "        model.add(Convolution2D(20, 10, 10, border_mode='same', activation='relu', name='conv1_2'))\n",
    "        model.add(MaxPooling2D((4,4), strides=(1,1)))\n",
    "        model.add(Dropout(0.5))\n",
    "        \n",
    "        model.add(Convolution2D(20, 10, 10, border_mode='same', input_shape=(10,10,20), activation='relu', name='conv2_0'))\n",
    "        model.add(Convolution2D(20, 5, 5, border_mode='same', activation='relu', name='conv2_1'))\n",
    "        model.add(Convolution2D(20, 10, 10, border_mode='same', activation='relu', name='conv2_2'))\n",
    "        model.add(MaxPooling2D((4,4), strides=(1,1)))\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(200, activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(100, activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(nb_classes, activation='sigmoid'))\n",
    "        \n",
    "        \n",
    "    if m ==15:\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Convolution2D(100,2,2, border_mode='same', \n",
    "                                input_shape=(20,10,10)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Convolution2D(50,2,2))\n",
    "#         model.add(Activation('relu'))\n",
    "#         model.add(Convolution2D(60,4,4))\n",
    "#         model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Flatten())\n",
    "\n",
    "        model.add(Dense(480))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(nb_classes))\n",
    "        model.add(Activation('sigmoid'))\n",
    "    \n",
    "    if m == 16:\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Convolution3D(20,9,9,18, border_mode='same',\n",
    "                                input_shape=(1,10,10,20)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Convolution3D(20,9,9,19))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "#         model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(480))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(nb_classes))\n",
    "        model.add(Activation('sigmoid'))\n",
    "        \n",
    "    if m ==18:\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(ZeroPadding2D((1,1),input_shape=(20,10,10)))\n",
    "        model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "        model.add(ZeroPadding2D((1,1)))\n",
    "        model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "        model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "        \n",
    "        model.add(ZeroPadding2D((1,1)))\n",
    "        model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
    "        model.add(ZeroPadding2D((1,1)))\n",
    "        model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
    "        model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "        model.add(ZeroPadding2D((1,1)))\n",
    "        model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "        model.add(ZeroPadding2D((1,1)))\n",
    "        model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "        model.add(ZeroPadding2D((1,1)))\n",
    "        model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "        model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "        model.add(ZeroPadding2D((1,1)))\n",
    "        model.add(Convolution2D(512, 2, 2, activation='relu'))\n",
    "        model.add(ZeroPadding2D((1,1)))\n",
    "        model.add(Convolution2D(512, 2, 2, activation='relu'))\n",
    "        model.add(ZeroPadding2D((1,1)))\n",
    "        model.add(Convolution2D(512, 2, 2, activation='relu'))\n",
    "        model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(4096, activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(4096, activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        \n",
    "        \n",
    "    if m ==20:\n",
    "        model = Sequential()\n",
    "        model.add(Convolution1D(100,10, activation='relu', input_shape=(100,10)))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Convolution1D(100,10, activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Convolution1D(100,10, activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Convolution1D(100,10, activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(MaxPooling1D((5)))\n",
    "        \n",
    "        \n",
    "        \n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(4000, activation='relu'))\n",
    "        \n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# np.random.seed(seed)\n",
    "estimators = []\n",
    "# estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_baseline, nb_epoch=100,\n",
    "                                          batch_size=512, verbose=1)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(y=y_data, n_folds=2, shuffle=True)#, random_state=seed)\n",
    "if not conv1d and dimOrdering == 'th':\n",
    "    X_data = np.swapaxes(X_data_,1,3)\n",
    "    X_data = np.swapaxes(X_data,2,3)\n",
    "    print(X_data.shape)\n",
    "elif conv1d:\n",
    "    print(X_data_.shape)\n",
    "    X_data = X_data_.reshape((X_data_.shape[0], X_data_.shape[1]* X_data_.shape[2], X_data_.shape[3]))\n",
    "    print(X_data.shape)\n",
    "else:\n",
    "    X_data = X_data_\n",
    "\n",
    "if conv3d:\n",
    "    X_data =  np.expand_dims(X_data, 1)\n",
    "    \n",
    "print(X_data.shape)\n",
    "results = cross_val_score(pipeline,X_data, y_data, cv=kfold)\n",
    "print(\"Standardized: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n",
    "\n",
    "json_string = model.to_json()\n",
    "model.save_weights('my_model_weights_2d_%d.h5'%m, overwrite=True)\n",
    "open('my_model_architecture%d.json'%m, 'w').write(json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1]\n",
      "(60, 50, 90)\n",
      "[10, 10, 10]\n"
     ]
    }
   ],
   "source": [
    "# we load a test case and the model\n",
    "\n",
    "# model = model_from_json(open('my_model_architecture%d.json'%m).read())\n",
    "# model.load_weights('my_model_weights_2d_%d.h5'%m)\n",
    "# model.compile(loss='binary_crossentropy',\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "print(data_spacing)\n",
    "nrrdData = nrrd.read(USERPATH + '/preprocessed_data/LabelMaps_%.2f-%.2f-%.2f/064/case.nrrd'%(tuple(data_spacing)))\n",
    "im = nrrdData[0]\n",
    "im = im[100:160,80:130,70:160]\n",
    "s = im.shape\n",
    "print(s)\n",
    "p=10\n",
    "print(patchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import pyprind\n",
    "# import sys\n",
    "# def findtips(N):\n",
    "#     '''\n",
    "#     Find the tip in the image by computing testing patches at every voxel position\n",
    "#     TODO: make this method more efficient\n",
    "#     '''\n",
    "#     p0, p1, p2 = patchsize\n",
    "#     xmiddle = s[0]//2\n",
    "#     ymiddle = s[1]//2\n",
    "#     zmiddle = s[2]//2\n",
    "    \n",
    "#     x0= xmiddle - xmiddle//N\n",
    "#     y0= ymiddle - ymiddle//N\n",
    "#     z0= zmiddle - zmiddle//N\n",
    "    \n",
    "#     xe= xmiddle + xmiddle//N\n",
    "#     ye= ymiddle + ymiddle//N\n",
    "#     ze= zmiddle + zmiddle//N\n",
    "    \n",
    "#     tips = []\n",
    "#     bar = pyprind.ProgBar(xmiddle//N*2, title='Find_tip', stream=sys.stdout)\n",
    "#     for xi in range(x0, xe-p0):\n",
    "#         for yi in range(y0, ye-p1):\n",
    "#             vols = [im[xi:xi+p0,yi:yi+p1,zi:zi+p2] for zi in range(z0,ze-p2)]\n",
    "#             # we normalize the data (centered on mean 0 and rescaled in function of the STD)\n",
    "#             volnorm = [ x-np.mean(x) for x in vols]\n",
    "#             volnorm2 = [x/np.std(x) for x in volnorm]\n",
    "#             cube = np.array(volnorm2)\n",
    "#             cube = np.swapaxes(cube, 1,3)\n",
    "# #             cube = np.swapaxes(cube, 2,3)\n",
    "#             if conv3d:\n",
    "#                 cube = np.expand_dims(cube,1)\n",
    "#             res = model.predict_proba(cube, batch_size=ze-p2-z0, verbose=False)\n",
    "#             indices = np.where(res[:,0]==1)\n",
    "#             # we add the coordinates of the center voxel of the patches that tested positive\n",
    "#             for z in indices[0]:\n",
    "#                 tips.append([xi+p0/2,yi+p1/2,z0+p2/2+z])\n",
    "#         bar.update()\n",
    "#     return tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyprind\n",
    "import sys\n",
    "def gettips(N):\n",
    "    '''\n",
    "    Find the tip in the image by computing testing patches at every voxel position\n",
    "    TODO: make this method more efficient\n",
    "    '''\n",
    "    p0, p1, p2 = patchsize\n",
    "    xmiddle = s[0]//2\n",
    "    ymiddle = s[1]//2\n",
    "    zmiddle = s[2]//2\n",
    "    \n",
    "    x0= xmiddle - xmiddle//N\n",
    "    y0= ymiddle - ymiddle//N\n",
    "    z0= zmiddle - zmiddle//N\n",
    "    \n",
    "    xe= xmiddle + xmiddle//N\n",
    "    ye= ymiddle + ymiddle//N\n",
    "    ze= zmiddle + zmiddle//N\n",
    "    \n",
    "    tips = []\n",
    "    bar = pyprind.ProgBar(xmiddle//N*2, title='Find_tip', stream=sys.stdout)\n",
    "    res = []\n",
    "    for xi in range(x0, xe-p0):\n",
    "        for yi in range(y0, ye-p1):\n",
    "            vols = [im[xi:xi+p0,yi:yi+p1,zi:zi+p2] for zi in range(z0,ze-p2)]\n",
    "            # we normalize the data (centered on mean 0 and rescaled in function of the STD)\n",
    "            volnorm = vols - np.mean(vols)\n",
    "            volnorm2 = volnorm/np.std(volnorm)\n",
    "#             volnorm = [ x-np.mean(x) for x in vols]\n",
    "#             volnorm2 = [x/np.std(x) for x in volnorm]\n",
    "            cube = np.array(volnorm2)\n",
    "            if not conv1d and dimOrdering == 'th':\n",
    "                cube = np.swapaxes(cube, 1,3)\n",
    "            if conv3d:\n",
    "                cube = np.expand_dims(cube,1)\n",
    "            if conv1d:\n",
    "                cube = cube.reshape(cube.shape[0], cube.shape[1]*cube.shape[2],cube.shape[3])\n",
    "            res.append(model.predict_proba(cube, batch_size=ze-p2-z0, verbose=False))\n",
    "        bar.update()\n",
    "    return res\n",
    "\n",
    "def findtips(res, prob):\n",
    "    N=1\n",
    "    p0, p1, p2 = patchsize\n",
    "    xmiddle = s[0]//2\n",
    "    ymiddle = s[1]//2\n",
    "    zmiddle = s[2]//2\n",
    "    \n",
    "    x0= xmiddle - xmiddle//N\n",
    "    y0= ymiddle - ymiddle//N\n",
    "    z0= zmiddle - zmiddle//N\n",
    "    \n",
    "    xe= xmiddle + xmiddle//N\n",
    "    ye= ymiddle + ymiddle//N\n",
    "    ze= zmiddle + zmiddle//N\n",
    "    \n",
    "    i = -1\n",
    "    tips = []\n",
    "    for xi in range(x0, xe-p0):\n",
    "        for yi in range(y0, ye-p1):\n",
    "            i+=1\n",
    "            indices = np.where(res[i][:,0]>=prob)\n",
    "            # we add the coordinates of the center voxel of the patches that tested positive\n",
    "            for z in indices[0]:\n",
    "                tips.append([xi+p0/2,yi+p1/2,z0+p2/2+z])\n",
    "    return tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find_tip\n",
      "0%                          100%\n",
      "[########################      ] | ETA: 00:00:02"
     ]
    }
   ],
   "source": [
    "# find the tips for patches with size p\n",
    "pred=gettips(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20154"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(pred))\n",
    "res = findtips(pred, 1)\n",
    "len(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of a labelmap from the voxel that tested positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mask = np.zeros(im.shape)\n",
    "for coord in res:\n",
    "    mask[int(coord[0]),int(coord[1]),int(coord[2])]=1.0\n",
    "nrrd.write('mask%d.nrrd'%m, mask)\n",
    "nrrd.write('im%d.nrrd'%m, im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f0b7b39aac8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk0AAADyCAYAAABQ+fHRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0b7b3b99e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%pylab inline\n",
    "# We display one axial slice\n",
    "Z = 13\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.imshow((np.clip(mask[:,:,Z]*255+im[:,:,Z]/2,a_min=0,a_max=200)).transpose(),  cmap='gray', interpolation='nearest')\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.imshow(im[:,:,Z].transpose(), cmap='gray', interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
